{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **OpenRouter API service**\n",
    "\n",
    "To get access to all the different models supported by OpenRouter, the first thing you need to do is to create an API key from OpenRouter. This API key is different from the one you can obtain from OpenAI. You need to add some money (the minimum is 4 dollars) that will be used to access GPT-4/GPT-4–32K/Other models. The pricings are the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Legend](./legend.png)\n",
    "![Prices](./GPT35-16k.png)\n",
    "![Prices](./GPT4.png)\n",
    "![Prices](./GPT4-32k.png)\n",
    "![Prices](./Claude.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What are tokens?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens can be thought of as pieces of words. Before the API processes the prompts, the input is broken down into tokens. These tokens are not cut up exactly where the words start or end - tokens can include trailing spaces and even sub-words. Here are some helpful rules of thumb for understanding tokens in terms of lengths:\n",
    "\n",
    "- 1 token ~= 4 chars in English\n",
    "- 1 token ~= ¾ words\n",
    "- 100 tokens ~= 75 words\n",
    "\n",
    "Or \n",
    "- 1-2 sentence ~= 30 tokens\n",
    "- 1 paragraph ~= 100 tokens\n",
    "- 1,500 words ~= 2048 tokens\n",
    "\n",
    "To get additional context on how tokens stack up, consider this:\n",
    "- Wayne Gretzky’s quote \"You miss 100% of the shots you don't take\" contains 11 tokens.\n",
    "- OpenAI’s charter contains 476 tokens.\n",
    "- The transcript of the US Declaration of Independence contains 1,695 tokens.\n",
    "\n",
    "Below I defined some token estimation functions which I will use to break down costs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_gpt = \"\"\"\n",
    "You will be given an engineering paper, which will serve as a knowledge base, and from that, you will generate JSON TRIPLETS, each consisting of an INSTRUCTION, an INPUT, and an OUTPUT, based on the content of the paper.\n",
    "Example:\n",
    "{\n",
    "  \"instruction\": \"What are the expected stress levels when the SMCT demo coil is powered to 15 kA in a 2L mirror configuration?\",\n",
    "  \"input\": \"With powering only SMCT demo coil to 15 kA (2L mirror) the peak stress in the SMCT structure in average increases to ~450 MPa with the maximum stress point of 599 MPa in the corner of second mid-plane block of the outer layer.\",\n",
    "  \"output\": \"When the SMCT demo coil is powered to 15 kA in a 2L mirror configuration, the average peak stress in the SMCT structure is expected to increase to approximately 450 MPa. The maximum stress point is anticipated to be 599 MPa, and this will be located in the corner of the second mid-plane block of the outer layer.\"\n",
    "}\n",
    "The instruction should pose a specific question or ask for an explanation concerning a technical aspect covered in the paper. The input should contain relevant and brief information, theories, or methodologies from the paper that pertain to the instruction. The output should provide a detailed answer to the instruction, adhering to the following guidelines: \n",
    "1. Incorporate mathematical formulations, equations, or symbols where applicable to substantiate your claims.\n",
    "2. Make use of precise technical terminology and definitions found in the paper.\n",
    "3. In your generations, never mention the paper or the document itself. Be absolute. For example, instead of saying \"the paper says\", say \"it is said\".\n",
    "\n",
    "Only one block should be generated per turn. For each turn, make the example slightly more complex than the last, while ensuring diversity.\n",
    "Make sure your samples are unique and diverse, yet high-quality and complex enough to train a well-performing model. You have a large enough context-length to generate high-quality samples, so please make use of it.\n",
    "The objective is to generate an output that is rigorous, comprehensive, and faithful to the technical and mathematical details of the paper.\n",
    "\n",
    "The paper is the following:\n",
    "\"\"\"\n",
    "\n",
    "temperature = .2\n",
    "number_of_examples = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_json = \"\"\"{\n",
    "\"instruction\": \"What are the characteristics of the UV-VIS arm's Atmospheric Dispersion Corrector (ADC) in the SOXS system?\",\n",
    "\"input\": \"The UV-VIS arm of the Common Path sub-system in the SOXS is characterized by a powered Atmospheric Dispersion Corrector (ADC). The ADC is composed of two counter-rotating quadruplets. Each of these quadruplets has two prisms and two lenses. ArcherOptx manufactured and delivered the quadruplets which are inserted in the respective barrels and motors. During the AIV process, a need for re-alignment was suggested to match required optical performances.\",\n",
    "\"output\": \"The Atmospheric Dispersion Corrector (ADC) in the UV-VIS arm of the SOXS system is a powered optical unit designed to correct for atmospheric dispersion. It consists of two counter-rotating quadruplets, each having two prisms and two lenses. These quadruplets are manufactured by ArcherOptx and are placed in their respective barrels and motors. The presence of powered optics in the ADC requires stringent alignment tolerances, and during the Alignment, Integration, and Verification (AIV) process, it was found necessary to re-align the ADC to meet the desired optical performance criteria.\"\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def estimate_tokens_by_characters(text):\n",
    "    num_chars = len(text)\n",
    "    return int(num_chars / 4)\n",
    "\n",
    "def estimate_tokens_by_words(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    num_words = len(words)\n",
    "    return int(num_words * (4 / 3))\n",
    "\n",
    "def estimate_tokens_by_sentences(text):\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    num_sentences = len(sentences)\n",
    "    return int(num_sentences * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "541 tokens required for the prompt\n"
     ]
    }
   ],
   "source": [
    "print(f'{estimate_tokens_by_characters(prompt_gpt)} tokens required for the prompt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 tokens required for each triplet\n"
     ]
    }
   ],
   "source": [
    "print(f'{estimate_tokens_by_characters(ref_json)} tokens required for each triplet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documents I scraped for this project have the following token counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2399 papers containing the acronym LPBF found.\n",
      "Average number of tokens per paper:  12246.91538140892\n",
      "Median number of tokens per paper:  11356\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "folder_path= '/Users/joanvelja/Documents/1000Kelvin/data/arxiv_mmd/processed'\n",
    "\n",
    "char_tokens = []\n",
    "sentence_tokens = []\n",
    "word_tokens = []\n",
    "\n",
    "for (dirpath, dirnames, filenames) in os.walk(folder_path):\n",
    "    for filename in filenames:\n",
    "        try:\n",
    "            with open(os.path.join(dirpath, filename), 'r') as f:\n",
    "                text = f.read()\n",
    "                if 'LPBF' or 'Laser Powder Bed Diffusion' in text:\n",
    "                    char_tokens.append(estimate_tokens_by_characters(text.strip().replace('\\n', ' ')))\n",
    "                    sentence_tokens.append(estimate_tokens_by_sentences(text.strip().replace('\\n', ' ')))\n",
    "                    word_tokens.append(estimate_tokens_by_words(text.strip().replace('\\n', ' ')))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "print(f'{len(char_tokens)} papers containing the acronym LPBF found.')\n",
    "print('Average number of tokens per paper: ', sum(char_tokens) / len(char_tokens))\n",
    "print('Median number of tokens per paper: ', sorted(char_tokens)[len(char_tokens) // 2])\n",
    "\n",
    "plt.hist(char_tokens, bins=100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we can see that the average token count is around 12000 tokens. This means that the average cost of a single prompting of the LLM is given by:\n",
    "$$\\text{PCost} = \\frac{12000 + 550}{1000} \\times \\text{Per 1k token cost} \\approx 12.55 \\times \\text{Per 1k token cost}$$\n",
    "$$\\text{GCost} = \\frac{\\text{Context Length}}{1000} \\times \\text{Per 1k token cost}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we can break down some costs:\n",
    "\n",
    "* **GPT-4**: Capped at 8k tokens, thus \n",
    "$$PCost = 12.55 \\times \\text{Per 1k token cost} = 12.55 \\times 0.03 = 0.3765 \\text{\\$ per prompt}$$\n",
    "$$GCost = \\frac{1200}{1000} \\times \\text{Per 1k token cost} = 1.200 \\times 0.06 = 0.072 \\text{\\$ per answer}$$\n",
    "$$\\text{Total Cost} = 0.3765 + 0,072 = 0.4485 \\text{ \\$ per paper}$$\n",
    "\n",
    "\n",
    "\n",
    "  Note that answer means multiple JSON responses, usually 3-4 (tested on GPT4). This implies that GPT generates around 1.2k tokens per answer. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* **GPT-4-32K**: Capped at 32k tokens, thus\n",
    "$$PCost = 12.55 \\times \\text{Per 1k token cost} = 12.55 \\times 0.06 = 0.753 \\text{\\$ per prompt}$$\n",
    "$$GCost = \\frac{1200}{1000} \\times \\text{Per 1k token cost} = 1.2 \\times 0.12 = 0.144 \\text{\\$ per answer}$$\n",
    "$$\\text{Total Cost} = 0.753 + 0.144 = 0.897 \\text{ \\$ per paper}$$\n",
    "\n",
    "Note that answer means multiple JSON responses, usually 3-4 (tested on GPT4). WARNING: I haven't tested GPT-4-32K's output length. It may output more than 1.2k tokens per answer. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Claude v1**: Capped at 100k tokens, thus\n",
    "$$PCost = 12.55 \\times \\text{Per 1k token cost} = 12.55 \\times 0.00163 \\approx 0.02 \\text{\\$ per prompt}$$\n",
    "$$GCost = \\frac{x}{1000} \\times \\text{Per 1k token cost} = x \\times 0.00551 \\approx 0.006x \\text{\\$ per answer}$$\n",
    "\n",
    "$$\\text{Total Cost} \\approx 0.02 + 0.006x \\text{ \\$ per paper}$$\n",
    "\n",
    "Note that Claude has a massive context length. Although the issue is that I have no access to the model (still unavailable in the EU).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Claude v2**: Capped at 100k tokens, thus\n",
    "$$PCost = 12.55 \\times \\text{Per 1k token cost} = 12.55 \\times 0.01102 \\approx 0.138 \\text{\\$ per prompt}$$\n",
    "$$GCost = \\frac{x}{1000} \\times \\text{Per 1k token cost} = x \\times 0.03268 \\approx 0.033x \\text{\\$ per answer}$$\n",
    "\n",
    "$$\\text{Total Cost} \\approx 0.138 + 0.033x \\text{ \\$ per paper}$$\n",
    "\n",
    "Note that Claude has a massive context length. Although the issue is that I have no access to the model (still unavailable in the EU)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* **GPT-3.5**: Capped at 16k tokens, thus\n",
    "$$PCost = 12.55 \\times \\text{Per 1k token cost} = 12.55 \\times 0.003  \\approx 0.038  \\text{\\$ per prompt}$$\n",
    "$$GCost = \\frac{1200}{1000} \\times \\text{Per 1k token cost} = 1.2 \\times 0.004 \\approx 0.005 \\text{\\$ per answer}$$\n",
    "  Note I am assuming that GPT-3 generates around 1.2k tokens per answer.\n",
    "\n",
    "$$\\text{Total Cost} \\approx 0.038 + 0.005 \\approx 0.043 \\text{ \\$ per paper}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recapping:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <table>\n",
    "    <tr>\n",
    "        <td> <b>Model</b> </td> <td> <b>Cost per paper</b></td> <td> <b>Total cost </b></td> \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> GPT-4 </td> <td> 0.4485 </td> <td> 1031.1015 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> GPT-4-32K </td> <td> 0.897 </td> <td> 2062.203 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> Claude v1 </td> <td> 0.02 + 0.006x </td> <td> 46 + 13.8x </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> Claude v2 </td> <td> 0.138 + 0.033x </td> <td> 317.4 + 76.2x </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> GPT-3.5 </td> <td> 0.043 </td> <td> 98.9 </td>\n",
    "    </tr>\n",
    "</table>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we got our OpenRouter API key, we need to install OpenAI python library and import some modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = .2\n",
    "number_of_examples = 3\n",
    "model = 'openai/gpt-4-32K'\n",
    "paper = \"\"\"\n",
    "**Zirconia UV-curable colloids for additive manufacturing via hybrid inkjet printing-stereolithography**\n",
    "\n",
    "Rosa M.\\({}^{1}\\)*, Barou C.\\({}^{2}\\) and Esposito V.\\({}^{1}\\)\n",
    "\n",
    "\\({}^{1}\\)DTU Energy, Technical University of Denmark, Riso Campus, Frederiksborgvej 399, 4000, Roskilde, Denmark.\n",
    "\n",
    "\\({}^{2}\\) ENSIL-ENSCI, Parc Ester Technopole, 16 rue Atlantis, 87068 Limoges Cedex, France.\n",
    "\n",
    "*corresponding author: masros@dtu.dk\n",
    "\n",
    "**Abstract**\n",
    "\n",
    "Currently, additive manufacturing of ceramics by stereolithography (SLA) is limited to single materials and by a poor thickness resolution that strongly depends on the ceramic particles-UV light interaction. Combining selective laser curing with inkjet printing represents a novel strategy to overcome these constrains. Nonetheless, this approach requires UV-curable inks that allow hardening of the printed material and sintering to high density. In this work, we report how to design an ink for inkjet printing of yttria stabilized zirconia (YSZ) which can be impressed by addition of UV-curable monomers. We especially show how the formulation of the inks and particularly the UV-monomer concentration impacts the printability and the UV-curing. This leads to prints that are resistant to solvent washing first and densify to 96% dense YSZ layers after sintering.\n",
    "\n",
    "**Keywords:** inkjet printing; UV-curable; zirconia; sintering.\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "Stereolithography (SLA) is an additive manufacturing technique where a 3D structure is built adding material in a layer-by-layer fashion, and consolidating selectively each layer with a laser scan or projected light [1]. In the case of ceramic materials, SLA can be used for sintering driedpowders with a high-intensity laser [2]. This method, known as direct selective laser sintering, is successfully applied to metals, but has shown limitations for ceramics due to residual porosity and low spatial resolution. Differently, the most typical SLA approach for these materials employs ceramic colloids dispersed in a UV-sensitive medium. With this method it is possible to shape complex ceramic structures by selective curing, and achieve full density after sintering [3]. In this configuration, the addition of material is carried out by a doctor blade, which distributes the uncured ceramic slurry, regulating the layer thickness. This mechanism introduces two main limitations in the SLA process, _i.e._ single-material manufacturing and a poor thickness resolution of ca. 10 \\(\\upmu\\)m. Other AM techniques, such as micro-extrusion, screen printing and fused deposition modeling, show even worse thickness and lateral resolutions, generally in the order of hundreds of microns [4, 5].\n",
    "\n",
    "On the other hand, inkjet printing consists in the deposition of small droplets of ink (typical volume 10-12 - 10-9 L) allowing local deposition of thin layers (\\(<\\)1 \\(\\upmu\\)m) of different materials [6]. Although inkjet printing has been used to print thick structures [7], its most successful application in ceramics lies in the deposition of thin layers. Therefore, due to the higher spatial resolution and multi-material capabilities of inkjet, combining the latter with SLA represents a possible solution to the aforementioned limitations in the additive manufacturing of ceramics.\n",
    "\n",
    "The feedstock materials used in SLA and inkjet printing possess, however, very different properties. In the case of SLA, highly viscous pastes are employed (solid loading ca. 50% vol), while, inkjet uses diluted, low viscosity inks (typical solid loading ca. 1% vol). Therefore, combining these two techniques requires UV-curable inks which allow reaching high density after sintering. UV-curable inks loaded with inorganic particles have been already developed [8]. However, they are designed to obtain a polymeric composite reinforced with ceramic particlesand not a precursor of a full-ceramic material [5, 8]. The formulation of a UV-curable ink dedicated to the fabrication of functional ceramic components by inkjet is thus missing.\n",
    "\n",
    "The addition of ceramic particles into a UV-curable ink introduces several issues that have to be addressed in the ink formulation. Especially, particles can increase the ink viscosity [9], becoming too high for inkjet printing, and scatter the UV-radiation hindering the UV-curing reaction [1, 10]. In this letter, we report the formulation of a UV-curable ink for inkjet printing of yttria-stabilized-zirconia (YSZ), a widely used ceramic in SLA manufacturing. We especially studied the impact of the polymer amount on the printability of the ink, hardening time and final microstructure, aiming to full dense sintered layers.\n",
    "\n",
    "## Experimental\n",
    "\n",
    "A detailed description of the materials and methods used is gives in the supplementary information.\n",
    "\n",
    "## Results\n",
    "\n",
    "TMPTA is a highly reactive monomer used in UV-curable inks as a cross-linking additive due to its triple acrylic functionality [11]. In this work, TMPTA was selected as a pure structural monomer to compensate for the presence of YSZ particles, which hinder cross-linking. In addition, due to its low molecular weight, the tendency of forming polymeric aggregates during the jetting is reduced [12], preventing clogging. IPA was selected as solvent for its low viscosity (1.95 mPa s), low surface tension (23 mN m-1) and low toxicity.\n",
    "\n",
    "A TMPTA-free 8YSZ ink with a solid loading of 8.7% wt was formulated as a reference for the preparation of the UV-curable ink. The formulation showed a single peak size distribution with a maximum at 200 nm and no particles above 400 nm.\n",
    "\n",
    "Two inks were formulated by adding 1.25% wt and 11% wt of TMPTA10 to the first ink. The volume ratios between TMPTA10 and 8YSZ particles in the two inks are 1:0.8 and 1:7.8. These two ratios were chosen to investigate the effect of the polymer amount on the printability and solvent resistance. Interestingly, the viscosity decreased from 2.81 mPa s to 2.71 mPa s and 2.12 mPa s upon the addition of 1.25% wt and 11% wt of TMPTA10 (Fig. 1). This result suggests that TMPTA10 might contribute to the dispersion of the 8YSZ particles, lowering the viscosity. After measuring density and surface tension, the printability parameters \\(Z\\) for these two inks were calculated. The ink printability is defined by equation (1): \\(Z=1/Oh=(\\rho\\cdot\\sigma\\cdot a)^{1/2}/\\eta\\), where \\(\\rho\\) is the density, \\(\\sigma\\) is the surface tension, \\(\\eta\\) is the viscosity, and \\(a\\) is the nozzle diameter. According to\n",
    "\n",
    "Figure 1: viscosity of the YSZ ink in IPA with different amounts of TMPTA10.\n",
    "\n",
    "the previous work by Derby [6] an ink is not jettable if Z\\(<\\)1, while multiple droplets are generated when Z\\(>\\)10.\n",
    "\n",
    "As reported in table 1, all the inks were formulated to be in the ideal range of Z for a proper jetting behavior. Viscosity, surface tension and density used to calculate the Z parameter of the three inks are summarized in table 1.\n",
    "\n",
    "\\begin{table}\n",
    "\\begin{tabular}{c c c c c c} \\hline\n",
    "**Ink** & **\\%wt of TMPTA** & **\\(\\eta\\) [mPa s]** & **\\(\\sigma\\) [mN m\\({}^{\\text{-1}}\\)]** & **\\(\\rho\\) [g/cm\\({}^{\\text{-3}}\\)]** & **Z** \\\\ \\hline\n",
    "**1** & - & 2.81 & 20.4 & 0.813 & 6.7 \\\\ \\hline\n",
    "**2** & 1.25 & 2.71 & 20.4 & 0.828 & 7.0 \\\\ \\hline\n",
    "**3** & 11.0 & 2.12 & 21.1 & 0.849 & 9.2 \\\\ \\hline \\end{tabular}\n",
    "\\end{table}\n",
    "Table 1: measured viscosity at 1000 s\\({}^{\\text{-1}}\\) (\\(\\eta\\)), surface tension (\\(\\sigma\\)), density (\\(\\rho\\)) and calculated printability Z.\n",
    "\n",
    "Figure 2: (a) single jetted droplets, (b), (c), (d) optimization of the printing process to achieve a uniform layer, (e) curing, (f) washing test with ethanol on a sample with 11% wt TMPTA10, and without TMPTA10 (g). The area covered with ethanol is on the right side of the dotted line.\n",
    "\n",
    "Figure 2(a) shows the typical jetting behavior obtained with the formulated inks. According to the calculated printability, the jetting of these inks shows a single round-shaped droplet. The inkjet printing process was optimized on a glass substrate by controlling the number deposited droplets per inch (DPI). Figure 2(b) shows that 100 DPI led to an incomplete coverage of the surface and it was possible to distinguish single splats with a diameter of ca. 130 \\(\\upmu\\)m. Using a resolution of 250 DPI the deposited layer was uniform (Fig. 2(c)), while with 400 DPI the splats overlapped, forming an uneven surface (Fig. 2(d)). Therefore, an areal density of 250 DPI was applied for printing multilayered samples that were cured with UV light, Fig. 2(e).\n",
    "\n",
    "To evaluate the solvent resistance and mimic the cleaning process for removing uncured material, the cured samples were observed after exposing them to ethanol, Fig. 2(f) and 2(g). Our results showed no difference in the solvent resistance of 20 minutes cured samples with a TMPTA10 content up to 1.25% wt. Fig. 2(g) shows that ethanol caused the immediate re-dispersion of the particles, which accumulate in the circled area. On the other hand, the inkjet-printed sample with 11% wt of TMPTA10 showed no significant changes in the structure of the film after the washing test. Notably, this result was achieved without operating in controlled atmosphere or adding oxygen scavengers, which are often needed to achieve curing in air.\n",
    "\n",
    "Due to the good resistance to ethanol of the formulation with 11%wt of TMPTA10, this ink was used for investigating the sintering of the 8YSZ prints. Squared samples made of 5 and 10 layers were printed on a green 3YSZ substrate and co-sintered in air after curing.\n",
    "\n",
    "Figure 3 reports the microstructure of the two samples: Fig. 3(a) and Fig. 3(b) show that the number of printed layers influenced the final structure of the sintered material. In particular, crack formation in the 10-layered sample is due to the shrinkage mismatch between support and deposition, which is caused by their different green densities, i.e. 50% vs 12%, respectively. This difference led to the formation of tensile stresses during sintering, which are more pronounced in the thicker 10-layered sample. As a consequence this sample cracked while the thinner 5-layered showed a crack-free microstructure. Fig. 3(a) also shows a non-uniform distribution of the porosity that is due to the coffee stain effect. In particular, this drying effect left regions with higher concentrations of TMPTA, which resulted in a local higher porosity. Nonetheless, the substrate coverage is complete with a maximum relative density up to 96% vol (Fig. 3(c)). High\n",
    "\n",
    "Figure 3: microstructural characterization after sintering. (a), (b) Top views of the 5-layered and 10-layered samples. (c), (d) Magnified microstructures of the 5-layered and 10-layered samples.\n",
    "\n",
    "densification, despite the low density, indicates a good particle packing as a result of well dispersed ceramic particles in the ink [13].\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "In this work, we report the formulation of a UV-curable ink for the fabrication of YSZ structures by a combination of SLA and inkjet printing. The developed ink allows stable jetting and contains 8.7% wt of 8YSZ particles and 11% wt of TMPTA as a monomer. The printed deposit is cured without operating in a controlled atmosphere or adding oxygen scavengers, becoming stable to ethanol. After curing, the 8YSZ particles can be sintered at 1295 \\({}^{\\circ}\\)C, reaching relative densities up to 96%. This result opens the possibility to apply the ink with 11%wt of TMPTA10 for patterning 8YSZ inkjet-printed films by selective curing.\n",
    "\n",
    "## Aknowledgements\n",
    "\n",
    "This project has partially received funding from the Fuel Cells and Hydrogen 2 Joint Undertaking under grant agreement No 700266. This Joint Undertaking receives support from the European Union's Horizon 2020 research and innovation program and Hydrogen Europe and N.ERGHY.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import random\n",
    "\n",
    "openai.api_base = \"https://openrouter.ai/api/v1\"\n",
    "openai.api_key = os.getenv(\"YOUR_OPENROUTER_API_KEY\")\n",
    "OPENROUTER_REFERRER = \"https://github.com/gitrepo\"\n",
    "\n",
    "\n",
    "def generate_example(model, prev_examples, paper, temperature=.5, content=prompt_gpt):\n",
    "    content = content.strip()  # Remove leading/trailing whitespace\n",
    "\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": content + paper\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    if len(prev_examples) > 0:\n",
    "        if len(prev_examples) > 10:\n",
    "            prev_examples = random.sample(prev_examples, 10)\n",
    "        for example in prev_examples:\n",
    "            messages.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": example\n",
    "            })\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        headers={\n",
    "            \"HTTP-Referer\": OPENROUTER_REFERRER\n",
    "        },\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=1354,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message['content']\n",
    "\n",
    "# Generate examples\n",
    "prev_examples = []\n",
    "for i in range(number_of_examples):\n",
    "    print(f'Generating example {i}')\n",
    "    example = generate_example(model, prev_examples, paper, temperature)\n",
    "    prev_examples.append(example)\n",
    "\n",
    "print(prev_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put our examples into a dataframe and turn them into a processed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_examples = ['''\n",
    "Hello, this is a test.\n",
    "{\n",
    "  \"name\": \"John\",\n",
    "  \"age\": 30,\n",
    "  \"city\": \"New York\",\n",
    "  \"details\": {\n",
    "    \"hobbies\": [\"reading\", \"swimming\"],\n",
    "    \"isMarried\": true\n",
    "  }\n",
    "}\n",
    "And this is the end of the first test.\n",
    "''', '''\n",
    "Multiple JSON objects are here.\n",
    "{\n",
    "  \"instruction\": \"Explain the concept of black holes.\",\n",
    "  \"input\": \"Black holes are regions of spacetime where gravity is so strong that not even light can escape.\",\n",
    "  \"output\": \"Black holes are areas in space where the gravitational pull is so intense that nothing, not even light, can escape them.\"\n",
    "}\n",
    "{\n",
    "  \"instruction\": \"What is the speed of light?\",\n",
    "  \"input\": \"The speed of light in a vacuum is approximately 299,792 kilometers per second.\",\n",
    "  \"output\": \"The speed of light is about 299,792 km/s when measured in a vacuum.\"\n",
    "}\n",
    "End of text.\n",
    "''',\n",
    "'''\n",
    "Complex with nested objects and arrays.\n",
    "{\n",
    "  \"employees\": [\n",
    "    {\"firstName\": \"John\", \"lastName\": \"Doe\"},\n",
    "    {\"firstName\": \"Anna\", \"lastName\": \"Smith\"},\n",
    "    {\"firstName\": \"Peter\", \"lastName\": \"Jones\"}\n",
    "  ],\n",
    "  \"department\": {\n",
    "    \"name\": \"Engineering\",\n",
    "    \"floor\": 7,\n",
    "    \"projects\": [\"Project A\", \"Project B\", {\"specialProject\": \"Project X\"}]\n",
    "  }\n",
    "}\n",
    "End of this test.\n",
    "''']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def find_json_objects(input_string):\n",
    "    \"\"\"\n",
    "    Find all JSON-like objects in a string.\n",
    "    \"\"\"\n",
    "    stack = []\n",
    "    json_objects = []\n",
    "    json_start = 0\n",
    "\n",
    "    for i, c in enumerate(input_string):\n",
    "        if c == '{':\n",
    "            stack.append('{')\n",
    "            if len(stack) == 1:\n",
    "                json_start = i\n",
    "        elif c == '}':\n",
    "            if stack:\n",
    "                stack.pop()\n",
    "                if len(stack) == 0:\n",
    "                    json_objects.append(input_string[json_start:i+1])\n",
    "\n",
    "    return json_objects\n",
    "\n",
    "def parse_json_from_string(input_string):\n",
    "    \"\"\"\n",
    "    Parse JSON objects from a given string and return them as a list of dictionaries.\n",
    "    \"\"\"\n",
    "    json_strings = find_json_objects(input_string)\n",
    "    parsed_json_list = []\n",
    "\n",
    "    for json_str in json_strings:\n",
    "        try:\n",
    "            parsed_json = json.loads(json_str)\n",
    "            parsed_json_list.append(parsed_json)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Failed to decode JSON: {e}\")\n",
    "            \n",
    "    return parsed_json_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsons = []\n",
    "for example in prev_examples:\n",
    "    try:\n",
    "        parsed_json = parse_json_from_string(example)\n",
    "        jsons.append(parsed_json)\n",
    "    except:\n",
    "        print('Failed to parse JSON')\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'John',\n",
       "  'age': 30,\n",
       "  'city': 'New York',\n",
       "  'details': {'hobbies': ['reading', 'swimming'], 'isMarried': True}}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_pairs(parsed_json_list):\n",
    "    instructions = []\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    for i, parsed_json in enumerate(parsed_json_list):\n",
    "        for key, value in parsed_json.items():\n",
    "            if key == 'instruction':\n",
    "                instructions.append(value)\n",
    "            elif key == 'input':\n",
    "                inputs.append(value)\n",
    "            elif key == 'output':\n",
    "                outputs.append(value)\n",
    "            else:\n",
    "                print(f'Unknown key: {key}')\n",
    "    \n",
    "    return instructions, inputs, outputs\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions, inputs, outputs = extract_pairs(jsons)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Instruction': instructions,\n",
    "    'Input': inputs,\n",
    "    'Output': outputs\n",
    "})\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "print('There are ' + str(len(df)) + ' successfully-generated examples. Here are the first few:')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Split into train and test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets, with 90% in the train set\n",
    "train_df = df.sample(frac=0.9, random_state=42)\n",
    "test_df = df.drop(train_df.index)\n",
    "\n",
    "# Save the dataframes to .jsonl files\n",
    "train_df.to_json('train.jsonl', orient='records', lines=True)\n",
    "test_df.to_json('test.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Install libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"transformers==4.34.0\" \"datasets==2.13.0\" \"peft==0.4.0\" \"accelerate==0.23.0\" \"bitsandbytes==0.41.1\" \"trl==0.4.7\" \"safetensors>=0.3.1\" --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instruct tune our model, we need to convert our structured examples into a collection of tasks described via instructions. We define a `formatting_function` that takes a sample and returns a string with our format instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(sample):\n",
    "\treturn f\"\"\"### Instruction:\n",
    "Use the Input below to create an instruction, which could have been used to generate the input using an LLM.\n",
    "\n",
    "### Input:\n",
    "{sample['response']}\n",
    "\n",
    "### Response:\n",
    "{sample['instruction']}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the recently introduced method in the paper \"QLoRA: Quantization-aware Low-Rank Adapter Tuning for Language Generation\" by Tim Dettmers et al. \n",
    "QLoRA is a new technique to reduce the memory footprint of large language models during finetuning, without sacrificing performance. The TL;DR; of how QLoRA works is:\n",
    "\n",
    "* Quantize the pre-trained model to 4 bits and freeze it.\n",
    "* Attach small, trainable adapter layers. (LoRA)\n",
    "* Finetune only the adapter layers while using the frozen quantized model for context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flash Attention is a an method that reorders the attention computation and leverages classical techniques (tiling, recomputation) to significantly speed it up and reduce memory usage from quadratic to linear in sequence length. It is based on the paper \"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\". \n",
    "The TL;DR; accelerates training up to 3x. Flash Attention is currently only available for Ampere (A10, A40, A100, ...) & Hopper (H100, ...) GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import torch; assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported for Flash Attention'\"\n",
    "%pip install ninja packaging\n",
    "!MAX_JOBS=4 pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "use_flash_attention = False\n",
    "\n",
    "# Hugging Face model id\n",
    "model_id = \"NousResearch/Llama-2-7b-hf\"  # non-gated\n",
    "# model_id = \"meta-llama/Llama-2-7b-hf\" # gated\n",
    "\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    use_cache=False,\n",
    "    use_flash_attention_2=use_flash_attention,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SFTTrainer supports a native integration with peft, which makes it super easy to efficiently instruction tune LLMs. We only need to create our LoRAConfig and provide it to the trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "# LoRA config based on QLoRA paper\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        r=64,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "\n",
    "# prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start our training we need to define the hyperparameters (TrainingArguments) we want to use. We will use the following hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"llama-7b-1000K\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=6 if use_flash_attention else 4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    disable_tqdm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have every building block we need to create our SFTTrainer to start then training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load path to jsonl\n",
    "#train_df = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 2048 # max sequence length for model and packing of the dataset\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_df,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    formatting_func=format_instruction,\n",
    "    args=args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "trainer.train() \n",
    "\n",
    "# save model\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training without Flash Attention enabled should take around 3hrs. The instance costs 1.212$/h which brings us to a total cost of 3.7$. The training with Flash Attention enabled should take around 2hrs on a g5.2xlarge. The instance costs 1.212$/h which brings us to a total cost of 2.6$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Test Model and run Inference**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training is done we want to run and test our model. We will use peft and transformers to load our LoRA adapter into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_flash_attention:\n",
    "    # unpatch flash attention\n",
    "    from utils.llama_patch import unplace_flash_attn_with_attn\n",
    "    unplace_flash_attn_with_attn()\n",
    "\n",
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "args.output_dir = \"llama-7b-1000K\"\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    args.output_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv1_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
